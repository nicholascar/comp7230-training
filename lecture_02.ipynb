{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guest Lecture COMP7230\n",
    "# Spatio-temporal data manipulation in Knowledge Graphs\n",
    "#### by Dr Nicholas Car\n",
    "\n",
    "This Notebook is the resource used to deliver a guest lecture for the [Australian National University](https://www.anu.edu.au)'s course [COMP7230](https://programsandcourses.anu.edu.au/2020/course/COMP7230): *Introduction to Programming for Data Scientists*. It is the second guest lecture in that series.\n",
    "\n",
    "Click here to run this lecture in your web browser:\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/nicholascar/comp7230-training/HEAD?filepath=lecture_02.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Lecture Outline\n",
    "1. [Notes about this training material](#sec-1)\n",
    "2. [Spatio-Temporal data use](#sec-2)\n",
    "3. [Multi-dimensional data](#sec-3)\n",
    "4. [S-T typology](#sec-4)\n",
    "5. [Semantic modelling](#sec-5)\n",
    "6. [Semantic S-T modelling](#sec-6)\n",
    "7. [Using semantic S-T data](#sec-7)\n",
    "8. [Conclusion / Suggested Actions](#sec-8)\n",
    "9. [References](#sec-9)\n",
    "\n",
    "The goal of this lecture is to convey some sense of the power of semantic Web data for datascience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec-1\"></a>\n",
    "## 1. Notes about this training material\n",
    "\n",
    "* this lecture is delivered using a [Jupyter Notebook](https://jupyter.org/)\n",
    "* see [Lecture 1's Notebook](lecture_01.ipynb) for notes on how to use this material in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec-2\"></a>\n",
    "## 2. Spatio-Temporal data use\n",
    "\n",
    "> _\"everything is somewhere...\"_\n",
    "\n",
    "everything is _some-when_ also?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Spatial\n",
    "* lots of familiar tools: some free\n",
    "    * Google Maps / OpenStreetMaps for simple web display\n",
    "    * PANDAS + maps for data sci\n",
    "    * ArcGIS / QGIS for desktop spatial data manipulation\n",
    "    * Oracle spatial or Postgres + PostGIS for spatial DB work\n",
    "\n",
    "* lots of data formats\n",
    "    * geometry serialisations (GeoJSON, WKT etc)\n",
    "    * spatial data files (SHAPE files, File GeoDatabases, CSV!)\n",
    "    * databases (Oracle, Postgres, even NSQL, like Mongo + GeoJSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Temporal\n",
    "Most software and database systems deal with temporality, at least to some extent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 S-T+\n",
    "* spatial/temporal data is almost always linked to non-spatial data for information\n",
    "* typical workflows involve separate spatial and non-spatial operations\n",
    "    * perhaps subsetting a dataset by area (spatial)\n",
    "    * perhaps then by time (temporal)\n",
    "    * then aggregating/transforming resultset into a final form (non-S-T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Example scenario: in QGIS\n",
    "\n",
    "Q: _find the average number of people, per dwelling, per suburb, in August 2021_\n",
    "\n",
    "Data might be presented as _people per dwelling per 'Mesh Block' for 2011 - 2021_\n",
    "\n",
    "Worflow might be:\n",
    "\n",
    "1. Filter out non-August 2021 data (temporal)\n",
    "2. Average people per dwelling, per MeshBlock (non-spatial)\n",
    "3. Intersect Mesh Blocks with suburbs (\"LGAs\")\n",
    "\n",
    "These steps are necessarily sequential if we can't represent all the dimensions of our data in one system. Even if we can, we need specialised functions for some dimensions: statistical (_average_), spatial (_within_, intersections) and temporal (also _within_).\n",
    "\n",
    "How would this scenario work with QGIS?\n",
    "\n",
    "![](lecture_resources/img/qgis-screenshot.png)\n",
    "**Figure 1**: Screenshot of the QGIS tool showing ACT\n",
    "\n",
    "In QGIS, we have lots of pre-loaded spatial operations, and also basic statistical operations such as 'average', but custom non-spatial operations aren't so easily catered for. \n",
    "\n",
    "QGIS and siliar systems also have scripting abilities:\n",
    "\n",
    "![](lecture_resources/img/qgis-scripting.png)\n",
    "**Figure 2**: Screenshot of the QGIS tool showing Python scripting. After <https://www.qgistutorials.com/en/docs/getting_started_with_pyqgis.html>\n",
    "\n",
    "```\n",
    "for f in layer.getFeatures():\n",
    "  geom = f.geometry()\n",
    "  print(f\"{f['name']}, {f['iata_code']}, {geom.asPoint().y()}, {geom.asPoint().x()}\")\n",
    "```\n",
    "\n",
    "However, typically, users export data to PANDAS, Tableau etc for detailed processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec-3\"></a>\n",
    "## 3. Multi-dimensional data\n",
    "\n",
    "* This demo data is about Meshblocks which are census counting areas within the Australian Bureau of Statistics' _Australian Statistitcal Geographies Standard_, which is online in Linked Data forma at <https://asgs.linked.fsdf.org.au>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Theory\n",
    "\n",
    "Consider this data:\n",
    "\n",
    "```\n",
    "Table: 2011\n",
    "-----------\n",
    "ID Ppl MB\n",
    "01 3   50055290000\n",
    "02 5   50055290000\n",
    "03 1   50055290000\n",
    "...\n",
    "\n",
    "Table 2016\n",
    "-----------\n",
    "ID Ppl MB\n",
    "11 3   50055290000\n",
    "12 3   50055290000\n",
    "...\n",
    "```\n",
    "\n",
    "We have _dimensions_ in columns.\n",
    "\n",
    "![](lecture_resources/img/data-dimensions.png)  \n",
    "**Figure 3**: Dimensions of the data in the _Example Scenario_ above\n",
    "\n",
    "> **_What are dimensions?_**\n",
    ">\n",
    "> **Business**: \"a set of data attributes pertaining to something of interest to a business\"\n",
    ">\n",
    "> **Scientific/modelling**: \"orthogonal projections of value\"\n",
    ">\n",
    "> Datasets are often realised in \"hypercubes\" with multiple dimensions\n",
    "\n",
    "* To perform any operations on these dimensions, S-T or other, we need to know about how those dimensions \"work\": their typology\n",
    "* If we know this, we can characterise functions per dimension\n",
    "* The more dimensions we can cater for in one place, the fewer sequential steps or system people will need to use to get the results they want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Example scenario: in a relational DB\n",
    "\n",
    "* Relational Databases easily cater for multi-dimensional data\n",
    "* SQLite is a relational DB in a single file\n",
    "    * likely the \"most implemented\" DB in the world: every Android and Apple phone\n",
    "    * don't be misled by the single file: it can scale to billions of entries...\n",
    "* Python contains code to work with the SQLite DB \"out of the box\"\n",
    "\n",
    "> NOTE: relational databases are awesome! My opinion is that most data science students would benefit from better relational DB training. RDB's are not particularly hard to use, are widely used and can do lots of things, but sometimes seem forgotten in the rush to demonstrate use of the latest Python data science package...\n",
    "\n",
    "Data for our scenario in RDB-ready form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID,NoPeople,ContainingMB,CensusYear\n",
      "01,3,50055290000,2011\n",
      "02,5,50055290000,2011\n",
      "03,1,50055290000,2011\n",
      "04,1,50055290000,2011\n",
      "05,3,50049040000,2011\n",
      "06,6,50049040000,2011\n",
      "07,3,50049040000,2011\n",
      "08,1,50049040000,2011\n",
      "09,5,50049040000,2011\n",
      "10,3,50049040000,2011\n",
      "11,3,50055290000,2016\n",
      "12,3,50055290000,2016\n",
      "13,3,50055290000,2016\n",
      "14,1,50055290000,2016\n",
      "15,2,50049040000,2016\n",
      "16,6,50049040000,2016\n",
      "17,3,50049040000,2016\n",
      "18,1,50049040000,2016\n",
      "19,4,50049040000,2016\n",
      "20,3,50049040000,2016\n",
      "21,3,50055290000,2021\n",
      "22,4,50055290000,2021\n",
      "23,1,50055290000,2021\n",
      "24,2,50055290000,2021\n",
      "25,2,50049040000,2021\n",
      "26,7,50049040000,2021\n",
      "27,3,50049040000,2021\n",
      "28,1,50049040000,2021\n",
      "29,5,50049040000,2021\n",
      "30,3,50049040000,2021\n"
     ]
    }
   ],
   "source": [
    "print(open(\"lecture_resources/lecture_02_dwellings.csv\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This data is in simple Comma-Seperated Value (CSV) form. There are many ways to load it into SQLite but I'll take a fairly simple approach below of just creating a table that matches the column content of the data and then I'll insert it into the table, by reading the file line-by-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened database successfully\n",
      "Table created successfully\n",
      "Rows in table: 0\n"
     ]
    }
   ],
   "source": [
    "# use standard Python libraries\n",
    "import csv\n",
    "import sqlite3\n",
    "\n",
    "# create an SQLite DB\n",
    "conn = sqlite3.connect('test.db')\n",
    "print(\"Opened database successfully\")\n",
    "\n",
    "# create a table within it\n",
    "conn.execute(\"DROP TABLE IF EXISTS dwellings;\")  # good practice to remove preexisting\n",
    "conn.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE dwellings\n",
    "    (ID INT PRIMARY KEY    NOT NULL,\n",
    "    NoPeople        INT    NOT NULL,\n",
    "    ContainingMB    INT   NOT NULL,\n",
    "    CensusYear      INT    NOT NULL);\n",
    "    \"\"\"\n",
    ")\n",
    "print(\"Table created successfully\")\n",
    "\n",
    "# check it's empty\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT COUNT(*) FROM dwellings;\")\n",
    "rows = cur.fetchall()\n",
    "print(f\"Rows in table: {rows[0][0]}\")  # should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read CSV data into table\n",
      "Rows in table: 30\n"
     ]
    }
   ],
   "source": [
    "# read CSV data, insert it into table\n",
    "with open(\"lecture_resources/lecture_02_dwellings.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)  # skip header\n",
    "    for field in reader:\n",
    "        conn.execute(\"INSERT INTO dwellings VALUES (?,?,?,?);\", field)\n",
    "print(\"Read CSV data into table\")\n",
    "\n",
    "# check there are 30 entries in table\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT COUNT(*) FROM dwellings;\")\n",
    "rows = cur.fetchall()\n",
    "print(f\"Rows in table: {rows[0][0]}\")  # should be 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that data loaded, let's naively query for the average number of people per MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of people per dwelling per Mesh Block in 2021:\n",
      "\n",
      "50049040000, 3.5\n",
      "50055290000, 2.5\n"
     ]
    }
   ],
   "source": [
    "# select only 2021 data, aggregate by containing Mesh Block\n",
    "cur = conn.cursor()\n",
    "cur.execute(\n",
    "    \"\"\"\n",
    "    SELECT ContainingMB, AVG(NoPeople) \n",
    "    FROM dwellings \n",
    "    WHERE CensusYear = 2021\n",
    "    GROUP BY ContainingMB\n",
    "    \"\"\"\n",
    ")\n",
    "rows = cur.fetchall()\n",
    "print(\"Average number of people per dwelling per Mesh Block in 2021:\")\n",
    "print()\n",
    "for row in rows:\n",
    "    print(f\"{row[0]}, {row[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Multi-dimensionality in SQL\n",
    "\n",
    "The query used above was:\n",
    "\n",
    "```\n",
    "SELECT ContainingMB, AVG(NoPeople) \n",
    "FROM dwellings \n",
    "WHERE CensusYear = 2021\n",
    "GROUP BY ContainingMB\n",
    "```\n",
    "\n",
    "Its dimensions:\n",
    "\n",
    "* `AVG(NoPeople)` - a statistical dimension operation\n",
    "* `CensusYear = 2021` - a temporal dimension operation\n",
    "* `GROUP BY ContainingMB` - using spatial information, MB, but a statistical operation\n",
    "\n",
    "We are able to pose this query because the dimentions here and their scales are simple!\n",
    "\n",
    "If we had different data, perhaps with more complex spatial information, we might have to have detailed spatial knowledge to use it ieffectively. E.g.:\n",
    "\n",
    "```\n",
    "SELECT AVG(NoPeople) \n",
    "FROM dwellings \n",
    "WHERE CensusYear = 2021\n",
    "AND ContainingMB NEXT TO MB1234\n",
    "GROUP BY ContainingMB\n",
    "```\n",
    "\n",
    "`NEXT TO ...` is not real SQL syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 SQL and Data Frames\n",
    "\n",
    "Relational DBs can store lots of information and make subsets of it that you want to report/graph available to you via SQL queries.\n",
    "\n",
    "A lot of data science is now done using Python tooling that manipulates data in-memory as this allows data scientists to run scripts, not servers. [PANDAS](https://pandas.pydata.org/about/) is such a tool.\n",
    "\n",
    "Exporting SQLite data to a PANDAS _DataFrame_ using Python is incredibly easy since PANDAS has SQL querying built in.\n",
    "\n",
    "Let's subset the data by getting just 2021 results using SQL and put it in a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  NoPeople  ContainingMB  CensusYear\n",
      "0  21         3   50055290000        2021\n",
      "1  22         4   50055290000        2021\n",
      "2  23         1   50055290000        2021\n",
      "3  24         2   50055290000        2021\n",
      "4  25         2   50049040000        2021\n",
      "5  26         7   50049040000        2021\n",
      "6  27         3   50049040000        2021\n",
      "7  28         1   50049040000        2021\n",
      "8  29         5   50049040000        2021\n",
      "9  30         3   50049040000        2021\n"
     ]
    }
   ],
   "source": [
    "# using our DB established above,\n",
    "# query the DB, load results into a dataframe\n",
    "import pandas as pd\n",
    "df = pd.read_sql(\"SELECT * FROM dwellings WHERE CensusYear = 2021\", conn)\n",
    "\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can subset the DB any way we like, create a DataFrame, do lots of calculatons with it as well as plotting etc., as we did in Lecture 1.\n",
    "\n",
    "Let's find the average number of people per dwelling, per Mesh Block using PANDAS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContainingMB\n",
       "50049040000    3.5\n",
       "50055290000    2.5\n",
       "Name: NoPeople, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"ContainingMB\")[\"NoPeople\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# remove up our DB to clean up\n",
    "conn.close()\n",
    "import os\n",
    "os.unlink(\"test.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec-4\"></a>\n",
    "## 4. S-T typology\n",
    "\n",
    "Lucky for us, the data to date has been pre-aligned per spatial & temporal feature (MB & year)\n",
    "\n",
    "Let's look in detail at spatial and temporal dimensions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Spatial\n",
    "\n",
    "* spatial typology can be called topology\n",
    "* _topology_ has a robust, formal set of models\n",
    "    * [Dimensionally Extended 9-Intersection Model (DE-9IM)](https://en.wikipedia.org/wiki/DE-9IM)\n",
    "    * [Simple Features](https://en.wikipedia.org/wiki/Simple_Features) [IS1, IS2]\n",
    "* many tools implement functions for these models\n",
    "    * PostGIS\n",
    "    * Oracle\n",
    "    * QGIS\n",
    "    * Python (e.g. the Shapely library\n",
    "    * GeoSPARQL implementations (next)\n",
    "\n",
    "Topological function examples:\n",
    "\n",
    "![](lecture_resources/img/TopologicSpatialRelarions2.png)\n",
    "**Figure 4**: Topological spatial relations examples\n",
    "By Krauss - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=21299138\n",
    "\n",
    "Example of [Shapely](https://pypi.org/project/Shapely/) implementing _contains_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shapely.geometry import Polygon, Point\n",
    "\n",
    "poly = Polygon([(0, 0), (1, 0), (1, 1), (0, 1), (0, 0)])\n",
    "point = Point(0.5, 0.5)  # contained\n",
    "poly.contains(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point2 = Point(1.5, 0.5)  # not contained\n",
    "poly.contains(point2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How are these functions used?\n",
    "    * for queries against spatial data collections\n",
    "        * try geometry-based searching on the Indigenous Data Network's Spatial Data Catalogue: <https://data.idnau.org/s>\n",
    "    * pre-calculating relations, e.g. the Feature [Statistical Area 2 404011096](https://linked.data.gov.au/dataset/asgsed3/SA2/404011096) has a number of pre-calculated topological relations [given in machine-readable form](http://asgs.linked.fsdf.org.au/dataset/asgsed3/collections/SA2/items/404011096?_profile=geo&_mediatype=text/turtle)\n",
    "    \n",
    "> _**ASIDE**: many domains need custom topological relations, such as hydrological catchments which have a special 'downstream' relation_ relevant for flood calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Temporal\n",
    "\n",
    "* temporal data is everywhere\n",
    "    * it's less visible than spatial, because it's simpler - 1D v. 2D/3D - and somewhat handled invisibly by lots of software\n",
    "    * still important to consider deeply\n",
    "* simpler topology: Allen relations [A83]\n",
    "\n",
    "![](lecture_resources/img/allen-relations.png)\n",
    "**Figure 5**: Allen relations\n",
    "_from https://www.ics.uci.edu/~alspaugh/cls/shr/allen.html_\n",
    "\n",
    "* implemented in many date/time tools\n",
    "    * e.g. Python's datetime library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "a = datetime.datetime(2021, 9, 7)\n",
    "b = datetime.datetime(2021, 10, 7)  # the day after a, above\n",
    "\n",
    "# before(a, b)\n",
    "print(a < b)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# after(a, b)\n",
    "print(a > b)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's mathematical more/less, greater/smaller functions interpreted as after/before for 1D time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec-5\"></a>\n",
    "## 5. Semantic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 RDF Recap\n",
    "\n",
    "_A short recapping of how the Resource Description Framework (RDF) works_\n",
    "\n",
    "For discussion:\n",
    "\n",
    "![](lecture_resources/img/example-graph-iris.jpg)  \n",
    "**Figure 6**: RDF image, from [the RDF Primer](https://www.w3.org/TR/rdf11-primer/)\n",
    "\n",
    "Note that:\n",
    "* _everything_ is \"strongly\" identified\n",
    "    * including all relationships\n",
    "    * unlike lots of related data\n",
    "* many of the identifiers resolve\n",
    "    * to more info (on the web)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Semantic Modelling\n",
    "\n",
    "We can use RDF to store both the _models_ we use and the _data_. This is different to relational and NoSQL DBs whos models are DB schema and separate from the data.\n",
    "\n",
    "An extension on the last lecture:\n",
    "\n",
    "![](lecture_resources/img/rdf-model-data.png)  \n",
    "**Figure 7**: RDF representing both model and data information\n",
    "\n",
    "With the model and the data in one place, we can use the them together. Using the model and data above, a pseudo code query to find \"Nick\" could be:\n",
    "\n",
    "> _Find all the objects of type `ex:Person` with the `ex:name` property equal to \"Nick\"_\n",
    "\n",
    "In [SPARQL](https://www.w3.org/TR/sparql11-query/) this would be:\n",
    "\n",
    "```\n",
    "PREFIX ex: <http://example.com/>\n",
    "\n",
    "SELECT *\n",
    "WHERE {\n",
    "    ?p a ex:Person ;\n",
    "        ex:name \"Nick\" ;\n",
    "    .\n",
    "}\n",
    "```\n",
    "\n",
    "To demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX ex: <http://example.com/>\n",
      "PREFIX orcid: <https://orcid.org/>\n",
      "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
      "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "\n",
      "#\n",
      "# model\n",
      "#\n",
      "ex:Thing\n",
      "    a rdfs:Class ;\n",
      ".\n",
      "\n",
      "ex:Person\n",
      "    a rdfs:Class ;\n",
      "    rdfs:subClassOf ex:Thing ;\n",
      ".\n",
      "\n",
      "ex:name\n",
      "    a rdf:Property ;\n",
      "    rdfs:domain ex:Person ;\n",
      "    rdfs:range ex:string ;\n",
      ".\n",
      "\n",
      "ex:string\n",
      "    a rdfs:Datatype ;\n",
      ".\n",
      "\n",
      "#\n",
      "# data\n",
      "#\n",
      "orcid:8742-7730\n",
      "    a ex:Person ;\n",
      "    ex:name \"Nick\" ;\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the data file content\n",
    "print(open(\"lecture_resources/lecture_02_person.ttl\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of triples in the graph is 9\n"
     ]
    }
   ],
   "source": [
    "# use the rdflib library\n",
    "from rdflib import Graph\n",
    "\n",
    "# load the RDF file into a graph\n",
    "g = Graph().parse(\"lecture_resources/lecture_02_person.ttl\")\n",
    "print(f\"The number of triples in the graph is {len(g)}\")  # should be 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ID of the node of type ex:Person with ex:name \"Nick\" is:\n",
      "\n",
      "* https://orcid.org/8742-7730\n"
     ]
    }
   ],
   "source": [
    "# query the graph\n",
    "print(f\"The ID of the node of type ex:Person with ex:name \\\"Nick\\\" is:\")\n",
    "print()\n",
    "for r in g.query(\n",
    "    \"\"\"\n",
    "    PREFIX ex: <http://example.com/>\n",
    "    \n",
    "    SELECT *\n",
    "    WHERE {\n",
    "        ?p a ex:Person ;\n",
    "            ex:name \"Nick\" ;\n",
    "        .\n",
    "    }\n",
    "    \"\"\"\n",
    "):\n",
    "    print(f\"* {r[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend the above query to find all the objects of type `ex:Thing`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ID of nodes of any type that is a sub class of ex:Thing:\n",
      "\n",
      "* https://orcid.org/8742-7730\n"
     ]
    }
   ],
   "source": [
    "# query the graph\n",
    "print(\"The ID of nodes of any type that is a sub class of ex:Thing:\")\n",
    "print()\n",
    "for r in g.query(\n",
    "    \"\"\"\n",
    "    PREFIX ex: <http://example.com/>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    \n",
    "    SELECT *\n",
    "    WHERE {\n",
    "        ?p rdf:type/rdfs:subClassOf ex:Thing .\n",
    "    }\n",
    "    \"\"\"\n",
    "):\n",
    "    print(f\"* {r[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `rdf:type/rdfs:subClassOf` in the query above is a 'path' query: one of the graph model superpowers!\n",
    "\n",
    "We have done this little demo to indicate how Knowlege Graph queries can navigate over edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Reasoning\n",
    "\n",
    "According to it's model's rules, which follow general set theory, any `x` that is of type `y` is also of type of any superclass of `y`. So for our data we can infer this relation:\n",
    "\n",
    "![](lecture_resources/img/rdf-reasoning.png)  \n",
    "**Figure 8**: RDF reasoning: using the model within the data\n",
    "\n",
    "We can execute software that \"builds\" data according to model rules to simplify queries. \n",
    "\n",
    "Building new data, in RDF Knowledge Graphs, often comes down to adding new nodes or edges to the graph.\n",
    "\n",
    "Let's add the edge for the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@prefix ex: <http://example.com/> .\n",
      "@prefix orcid: <https://orcid.org/> .\n",
      "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
      "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
      "\n",
      "ex:Person a rdfs:Class ;\n",
      "    rdfs:subClassOf ex:Thing .\n",
      "\n",
      "ex:Thing a rdfs:Class .\n",
      "\n",
      "ex:name a rdf:Property ;\n",
      "    rdfs:domain ex:Person ;\n",
      "    rdfs:range ex:string .\n",
      "\n",
      "orcid:8742-7730 a ex:Person,\n",
      "        ex:Thing ;\n",
      "    ex:name \"Nick\" .\n",
      "\n",
      "ex:string a rdfs:Datatype .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.update(\n",
    "    \"\"\"\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    \n",
    "    INSERT {\n",
    "        ?x rdf:type ?z .\n",
    "    }\n",
    "    WHERE {\n",
    "        ?x rdf:type ?y .\n",
    "        ?y rdfs:subClassOf ?z .\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# show the updated data, look for orcid:8742-7730's types\n",
    "print(g.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that `orcid:8742-7730` is now of type `ex:Thing` as well as the original `ex:Person`.\n",
    "\n",
    "Now we can use the simpler query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ID of nodes of any type that is a sub class of ex:Thing: \n",
      "\n",
      "* https://orcid.org/8742-7730\n"
     ]
    }
   ],
   "source": [
    "print(\"The ID of nodes of any type that is a sub class of ex:Thing: \")\n",
    "print()\n",
    "for r in g.query(\n",
    "    \"\"\"\n",
    "    PREFIX ex: <http://example.com/>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    \n",
    "    SELECT *\n",
    "    WHERE {\n",
    "        ?p rdf:type ex:Thing .\n",
    "    }\n",
    "    \"\"\"\n",
    "):\n",
    "    print(f\"* {r[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec-6\"></a>\n",
    "## 6. Semantic S-T modelling\n",
    "\n",
    "_In this section we are going to step through common Semantic Web ways of modelling spatiality and temporality, based on the typologies described above. Just not that there are many ways of doing things in the Semantic Web and while these methods are common, there are other ways for other purposes._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 GeoSPARQL & OWL Time\n",
    "\n",
    "We have well-known, standardised and free, _ontologies_ for spatial and temporal domains:\n",
    "\n",
    "* GeoSPARQL: [GSP] - spatial\n",
    "* OWL Time: [TIME] - temporal\n",
    "\n",
    "Using them and representing both mdoels and data:\n",
    "\n",
    "![](lecture_resources/img/semantic-st-modelling.png)  \n",
    "**Figure 9**: (left) GeoSPARQL `geo:Feature` and `geo:Geometry` classes and their relations & OWL Time `time:TemporalEntity` (no `Feature` equivalent); (center) A flood, `ex:flood-1` modelled with both spatial and temporal properties; (right) the inference that `ex:flood-1` is a `geo:Feature`, from its use of `geo:hasGeometry`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Typology\n",
    "\n",
    "GeoSPARQL & OWL TIME contain spatial and temporal relations as per _Simple Features_ and _Allen Relations_ respectively:\n",
    "\n",
    "Ontology | Relation | Property\n",
    ":--- | --- | ---\n",
    "GeoSPARQL | SF within | `geo:sfWithin`\n",
    "&nbsp; | SF contains | `geo:sfContains`\n",
    "&nbsp; | EH covers | `geo:ehCovers`\n",
    "OWL Time | before | `time:before`\n",
    "&nbsp; | after | `time:after`\n",
    "&nbsp; | in | `time:intervalIn`\n",
    "\n",
    "Following on from Figure 9: given that any object with spatial relations is a `geo:Feature`, if `ex:flood-1 geo:ehCovers ex:school-grounds-x` then `ex:flood-1 rdfs:type geo:Feature` .\n",
    "\n",
    "Both ontologies contain other property rules, e.g. `time:before` transitivity & `time:before`/`time:after` inverse. Let's perform some simple reasoning using these two OWL Time rules:\n",
    "\n",
    "Given:\n",
    "```\n",
    "ex:IronAge time:before ex:Renaissance .\n",
    "\n",
    "ex:20thCentury time:after ex:Renaissance .\n",
    "```\n",
    "Using `time:before`/`time:after` inverse:\n",
    "```\n",
    "ex:IronAge time:before ex:Renaissance .\n",
    "\n",
    "ex:Renaissance time:before ex:20thCentury .\n",
    "```\n",
    "Using `time:before` transitivity:\n",
    "```\n",
    "ex:IronAge time:before ex:20thCentury .\n",
    "```\n",
    "So a Semantic database loaded with OWL Time's rules would automatically calculate the last statement from the given statements and thus you could query the database like this and get a result:\n",
    "\n",
    "```\n",
    "PREFIX ex: <http://example.com/>\n",
    "PREFIX time: <http://www.w3.org/2006/time#>\n",
    "\n",
    "SELECT * \n",
    "WHERE {\n",
    "    ex:IronAge time:before ?x \n",
    "}\n",
    "```\n",
    "\n",
    "...and you would get: `?x` = `ex:Renaissance`, `ex:20thCentury`. The results from given and calculated data are not normally differentiated, although many systems allow you to indicate given/calculated if you want.\n",
    "\n",
    "Let's run that example in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# parse given data into a graph\n",
    "from rdflib import Graph\n",
    "\n",
    "g = Graph().parse(\n",
    "    data=\"\"\"\n",
    "    PREFIX ex: <http://example.com/>\n",
    "    PREFIX time: <http://www.w3.org/2006/time#>\n",
    "\n",
    "    ex:IronAge time:before ex:Renaissance .\n",
    "\n",
    "    ex:20thCentury time:after ex:Renaissance .\n",
    "    \"\"\",\n",
    "    format=\"turtle\"\n",
    ")\n",
    "# print no. triples in the graph\n",
    "print(len(g))  # should be 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# manually run each OWL Time rule\n",
    "# time:before/time:after inverse\n",
    "g.update(\n",
    "    \"\"\"\n",
    "    PREFIX time: <http://www.w3.org/2006/time#>\n",
    "\n",
    "    INSERT {\n",
    "        ?y time:after ?x .\n",
    "        ?n time:before ?m .\n",
    "    }\n",
    "    WHERE {\n",
    "        ?x time:before ?y .\n",
    "        ?m time:after ?n .\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "# print no. triples in the graph\n",
    "print(len(g))  # should be 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# time:before transitivity\n",
    "g.update(\n",
    "    \"\"\"\n",
    "    PREFIX time: <http://www.w3.org/2006/time#>\n",
    "\n",
    "    INSERT {\n",
    "        ?x time:before ?y\n",
    "    }\n",
    "    WHERE {\n",
    "        ?x time:before+ ?y .\n",
    "    }\n",
    "    \"\"\"\n",
    ")\n",
    "# print no. triples in the graph\n",
    "print(len(g))  # should be 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@prefix ex: <http://example.com/> .\n",
      "@prefix time: <http://www.w3.org/2006/time#> .\n",
      "\n",
      "ex:IronAge time:before ex:20thCentury,\n",
      "        ex:Renaissance .\n",
      "\n",
      "ex:20thCentury time:after ex:Renaissance .\n",
      "\n",
      "ex:Renaissance time:after ex:IronAge ;\n",
      "    time:before ex:20thCentury .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print total graph\n",
    "print(g.serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http://example.com/Renaissance>\n",
      "<http://example.com/20thCentury>\n"
     ]
    }
   ],
   "source": [
    "# ask the query\n",
    "for r in g.query(\n",
    "        \"\"\"\n",
    "        PREFIX ex: <http://example.com/>\n",
    "        PREFIX time: <http://www.w3.org/2006/time#>\n",
    "\n",
    "        SELECT *\n",
    "        WHERE {\n",
    "            ex:IronAge time:before ?x\n",
    "        }\n",
    "        \"\"\"\n",
    "    ):\n",
    "    print(f\"<{r[0]}>\")  # should be 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Functions\n",
    "\n",
    "GeoSPARQL contains functions to calculate relations, e.g. `geo:contains`, using data, as per the example in 3.1. The function `geof:contains()` is used like this, to find all the things that `ex:feature-x` contains, spatially:\n",
    "\n",
    "```\n",
    "PREFIX geo: <http://www.opengis.net/ont/geosparql#>\n",
    "\n",
    "SELECT ?contained\n",
    "WHERE {\n",
    "    ex:feature-x geo:hasGeometry ?ga ;    \n",
    "    ?contained geo:hasGeometry ?gb ;\n",
    "    \n",
    "    FILTER geo:contains(?ga, ?gb)\n",
    "}\n",
    "```\n",
    "\n",
    "OWL Time doesn't contain equivalent functions... but I've made some [TMF]! Let's see...\n",
    "\n",
    "Example data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(open(\"lecture_resources/lecture_02_tf.ttl\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above data is shown in Figure 8.\n",
    "\n",
    "![](lecture_resources/img/tf-rdf.png)  \n",
    "**Figure 10**: A set of `time:TemporalEntity` instances with declared and inferrable/calculable relations (in orange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "from timefuncs import TFUN\n",
    "\n",
    "g = Graph().parse(\"lecture_resources/lecture_02_tf.ttl\")\n",
    "print(f\"The number of triples in the graph is {len(g)}\")  # should be 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in g.query(\n",
    "        \"\"\"\n",
    "        PREFIX tfun: <https://w3id.org/timefuncs/>\n",
    "        \n",
    "        SELECT ?x ?y\n",
    "        WHERE {\n",
    "            ?x a ?c1 .\n",
    "            ?y a ?c2 .\n",
    "        \n",
    "            FILTER tfun:isBefore(?x, ?y)\n",
    "        }\n",
    "        \"\"\"\n",
    "):\n",
    "    print(f\"{r[0]} is before {r[1]}\")  # should be 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this query uses both graph path following logic and numerical (date) calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec-7\"></a>\n",
    "## 7. Using semantic S-T data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Semantically Enhanced Dwelling data\n",
    "\n",
    "A \"semantically enhanced\" version of the dwellings CSV used in the SQLite example above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(open(\"lecture_resources/lecture_02_dwellings.ttl\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhanced in that:\n",
    "\n",
    "* element IDs have been universalised with the addition of globally unique, Internet, namespaces:\n",
    "    * 50055290000 &rarr; `https://linked.data.gov.au/dataset/asgsed3/MB/50055290000`\n",
    "    * using a reference to the namespace, `mb` == `https://linked.data.gov.au/dataset/asgsed3/MB/`:\n",
    "        * 50055290000 &rarr; `mb:50055290000`\n",
    "\n",
    "> _This namespace and the item is rea real and it resolves! Try: https://linked.data.gov.au/dataset/asgsed3/MB/50055290000_\n",
    "\n",
    "* simple data types turned into RDF nodes with their own properties:\n",
    "    * \"2016\" &rarr; `ex:year-2016`\n",
    "* spatial & temporal topology hase been introduced:   \n",
    "    * ```\n",
    "    mb:50055290000\n",
    "        a geo:Feature ;\n",
    "        geo:sfTouches mb:50049040000 ;\n",
    "    .\n",
    "    ```\n",
    "\n",
    "    * ```\n",
    "    ex:year-2021 \n",
    "        a time:temporalEntity ;\n",
    "        time:after ex:year-2016 ;\n",
    "        time:inXSDgYear 2021 ;\n",
    "    .\n",
    "    ```\n",
    "* data relationships (the model used) are given, see next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The header and first data row from the CSV version of the data looked like this:\n",
    "\n",
    "```\n",
    "ID,NoPeople,ContainingMB,CensusYear\n",
    "01,3,50055290000,2011\n",
    "```\n",
    "\n",
    "The Semantic version's first row equivalent looks like this:\n",
    "\n",
    "```\n",
    "ex:obs-01\n",
    "    a ex:Observation ;\n",
    "    ex:containingMB mb:50055290000 ;\n",
    "    ex:hasNoPeople 3 ;\n",
    "    time:hasTime ex:year-2011 .\n",
    "```\n",
    "\n",
    "The informal linking of data in columns, in the CSV \"3\" to \"NoPeople\", via the column heading, has been replaced with a formal, defined, relationship which here is `ex:hasNoPeople 3`. The `ex:hasNoPeople` property is defined now (not left to a human's interpretation of the text \"NoPeople\") and added to the RDF dwellings file like this:\n",
    "\n",
    "```\n",
    "ex:hasNoPeople\n",
    "    a rdf:Property ;\n",
    "    rdfs:label \"has nbumber of people\" ;\n",
    "    rdfs:domain ex:Observation ;\n",
    "    rdfs:range xsd:integer ;\n",
    ".\n",
    "```\n",
    "\n",
    "So `ex:hasNoPeople` 'means' a property which is labelled \"has number of people\" and which must (only) be applied to records fo type `ex:Observation` - a so-called 'domain' constraint - and which can only result in an `xsd:integer` - the standard integer data type, a 'range' constraint. What _has number of people_ actually means can be explained in further descriptions and even relations to other similar properties, perhaps general counts of things, if desired.\n",
    "\n",
    "Our Semantic dwellings can be visually presented as per Fig. 9 below:\n",
    "\n",
    "![](lecture_resources/img/sem-dwellings-02.png)\n",
    "**Figure 11**: The dimensions of the semantically enhanced version of the dwellings data. The property indicating the number of people counted per observation, per dwelling, is a statistical property. Spatial and Temporal dimensions are dealt with via complex objects of an _Observed feature_ and an _Observed time period_ respectively, related to the Observation. The specific values of _Mesh Block_ and _Year_ for the _Observed feature_ and an _Observed time period_ are not all we have: we also have relations to other features and time periods, e.g. Mesh Block 50055290000 touching MeshBlock 50049040000 which is known via other properties of the Mesh Blocks themselves.\n",
    "\n",
    "### 7.2 Extending the Dwelling data\n",
    "\n",
    "We can easily extend semantic data both by adding more things in new dimensions of established types - perhaps incorporating information about the methods used to obtain observations from a surveying ontology - and by defining our own properties and objects if no established models exist that meet our needs. Fig. 10 shows potential new properties (simple data) or relations (complex objects) added at two different points to our data: the Observation element and the Observed Feature.\n",
    "\n",
    "![](lecture_resources/img/sem-dwellings-03.png)\n",
    "**Figure 12**: New information - both model and data elements - added at various points to the data in Fig. 9.\n",
    "\n",
    "> _ASIDE: Remember that with Semantic Web data, we are often able to directly reuse existing datasets. In this example, there is indeed a wealth of Semantic Web information about Mesh Blocks in the [ASGS Ontology](https://linked.data.gov.au/def/asgs) and [ASGS 2016](https://linked.data.gov.au/dataset/asgs2016) datasets that we could extend our dwellings data with. The [Loc-I project](https://www.ga.gov.au/locationindex) provides a number of Australian national datasets in Semantic web form, including multiple versions of the ASGS._\n",
    "\n",
    "A dimension of data that is often of great relevance is that of provenance: the how and when of data's production. There is a well-known provenance ontology that we could use to provide us with properties and objects to describe how our dwellings data was collected and processed. Fig. 11 shows our data with some potential provenance extensions.\n",
    "\n",
    "![](lecture_resources/img/sem-dwellings-04.png)\n",
    "**Figure 13**: Provenance dimension information for the Observation and Observed Feature of our dwellings data indicated\n",
    "\n",
    "### 7.3 Semantic multi-dimensional data querying\n",
    "\n",
    "With our non-semantic data loaded into the SQLite relational database, we formulated the following SQL query that utilised multiple dimensions:\n",
    "```\n",
    "SELECT ContainingMB, AVG(NoPeople)\n",
    "FROM dwellings\n",
    "WHERE CensusYear = 2021\n",
    "GROUP BY ContainingMB\n",
    "```\n",
    "\n",
    "With our semantic data, we can emulate this query in SPARQL like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50055290000, 2.5\n",
      "50049040000, 3.5\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "# load the data\n",
    "g = Graph().parse(\"lecture_resources/lecture_02_dwellings.ttl\")\n",
    "# query it\n",
    "q = \"\"\"\n",
    "    PREFIX ex: <http://example.com/>\n",
    "    PREFIX time: <http://www.w3.org/2006/time#>\n",
    "\n",
    "    SELECT ?mb (AVG(?people) AS ?avg_people)\n",
    "    WHERE {\n",
    "        ?obs ex:hasNoPeople ?people ;\n",
    "             time:hasTime ex:year-2021 ;\n",
    "             ex:containingMB ?mb .\n",
    "    }\n",
    "    GROUP BY ?mb\n",
    "    \"\"\"\n",
    "for r in g.query(q):\n",
    "    print(f\"{r[0].split('/')[-1]}, {r[1]}\")  # should be the average we have seen before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This SPARQL query gives the same results as the SQL query and obtains them in approximately the same way by filtering the results according to a required year (2021) and grouping them by Mesh Block, however the result gives universally unique identifiers for the Mesh Blocks and we ar filtering not by a year value, \"2021\" but by association to a year object, `ex:year-2021`. Thus we won't get confused by Mesh block from different versions of the ASGS dataset and we can look them up (use the web link) to get more information about them, perhaps area. We can also use Mesh Block spatial and year temporal relations.\n",
    "\n",
    "For example, this next query gets the same results again but uses spatial-temporal typology to get there, rather than specific values, i.e. results for _any year after 2016_, rather than _year = 2021_ and _any Mesh Block that touches MB 50055290000_ rather than _MB = 50055290000_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50049040000, 3.5\n"
     ]
    }
   ],
   "source": [
    "q2 = \"\"\"\n",
    "    PREFIX ex: <http://example.com/>\n",
    "    PREFIX geo: <http://www.opengis.net/ont/geosparql#>\n",
    "    PREFIX mb: <https://linked.data.gov.au/dataset/asgsed3/MB/>\n",
    "    PREFIX time: <http://www.w3.org/2006/time#>\n",
    "\n",
    "    SELECT ?mb (AVG(?people) AS ?avg_people)\n",
    "    WHERE {\n",
    "        ?obs ex:hasNoPeople ?people ;\n",
    "             time:hasTime ?year ;\n",
    "             ex:containingMB ?mb .\n",
    "\n",
    "        ?year time:after ex:year-2016 .\n",
    "        ?mb geo:sfTouches mb:50055290000 .\n",
    "    }\n",
    "    GROUP BY ?mb\n",
    "    \"\"\"\n",
    "for r in g.query(q2):\n",
    "    print(f\"{r[0].split('/')[-1]}, {float(r[1]):.1f}\")  # same results as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 7.4 SPARQL and dataframes\n",
    "\n",
    "As with relational DB data accessed via SQL: we can easily export data using SPARQL queries into PANDAS dataframes. \n",
    "\n",
    "~There is no general SPARQL&rarr;dataframe (yet!) but we can just translate to CSV like this:~ - statement from 2021\n",
    "\n",
    "We now have: https://github.com/RDFLib/sparqlwrapper/blob/master/SPARQLWrapper/sparql_dataframe.py\n",
    "\n",
    "...but we can just translate to CSV like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           MeshBlock  NoPeople\n",
      "0  https://linked.data.gov.au/dataset/asgsed3/MB/...         3\n",
      "1  https://linked.data.gov.au/dataset/asgsed3/MB/...         4\n",
      "2  https://linked.data.gov.au/dataset/asgsed3/MB/...         1\n",
      "3  https://linked.data.gov.au/dataset/asgsed3/MB/...         2\n",
      "4  https://linked.data.gov.au/dataset/asgsed3/MB/...         2\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "    PREFIX ex: <http://example.com/>\n",
    "    PREFIX geo: <http://www.opengis.net/ont/geosparql#>\n",
    "\n",
    "    SELECT ?mb ?people\n",
    "    WHERE {\n",
    "        ?obs\n",
    "            ex:hasNoPeople ?people ;\n",
    "            ex:containingMB ?mb ;\n",
    "            time:hasTime ex:year-2021 ;\n",
    "        .\n",
    "    }\n",
    "    \"\"\"\n",
    "csv = \"MeshBlock,NoPeople\\n\"\n",
    "\n",
    "# use the query above\n",
    "for r in g.query(q):\n",
    "    csv += f\"{r['mb']},{r['people']}\\n\"\n",
    "\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "sparql_df = pd.read_csv(StringIO(csv))\n",
    "\n",
    "print(sparql_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"sec-8\"></a>\n",
    "## 8. Conclusions / Suggested Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than conclude with statements, I'm going to make suggestions.\n",
    "\n",
    "* **learn about typologies relevant to your domain**\n",
    "    * knowledge of these assists you in considering new possibilities\n",
    "    * spatial, temporal, provenance, governance & partiness (how data is bundled into dataset) are all of quite universal interest and all have their own special relationships\n",
    "* **learn a bit about relational databases**\n",
    "    * if they are new to you, you may learn about multi-dimensional handing in SQL\n",
    "    * consider using SQLite as you can use it easily with Python and thus with PANDAS, Matplotlib and so on\n",
    "* **explore the RDFLib toolkit**\n",
    "    * since RDFLib is in Python and works well with PANDAS etc., it's a natural place to start using Semantic Web data for data sience\n",
    "    * https://rdflib.readthedocs.io\n",
    "* **find ready-to-go Semantic data**\n",
    "    * some starting points:\n",
    "        * ASGS: https://linked.data.gov.au/dataset/asgsed3\n",
    "        * GNAF: https://linked.data.gov.au/dataset/gnaf\n",
    "        * Geofabric [https://linked.data.gov.au/dataset/geofabric](https://gnaf.linked.fsdf.org.au/dataset/gnaf/collections)\n",
    "        * Indigenous Spatial Data: https://data.idnau.org/s\n",
    "        * Australian Government Linked Data Working Group: https://www.linked.data.gov.au/\n",
    "* **ask the Government for more Semantic Web data**\n",
    "    * I think Semantic Web data is the best form of data for utility and transparency\n",
    "    * government claims to make data best available, so it should be in Semantic Web form\n",
    "    * Try hassling the National Data Commissioner: https://www.datacommissioner.gov.au"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sec-9\"></a>\n",
    "## 9. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **[A83]**: Allen, James F. \"Maintaining knowledge about temporal intervals\". Communications of the ACM 26(11) pp.832-843, Nov. 1983.\n",
    "* **[DEM9]**: Clementini E., Di Felice P., van Oosterom P. (1993) \"A small set of formal topological relationships suitable for end-user interaction\". In: Abel D., Chin Ooi B. (eds) Advances in Spatial Databases. SSD 1993. Lecture Notes in Computer Science, vol 692. Springer, Berlin, Heidelberg. https://doi.org/10.1007/3-540-56869-7_16\n",
    "* **[GSP]**: Open Geospatial Consortium \"OGC GeoSPARQL - A Geographic Query Language for RDF Data\". Implementation standard (draft). https://opengeospatial.github.io/ogc-geosparql/geosparql11/spec.html\n",
    "* **[IS1]**: International Organization for Standardization \"ISO 19125-1:2004 Geographic information -- Simple feature access -- Part 1: Common architecture\". International standard.\n",
    "* **[IS2]**: International Organization for Standardization \"ISO 19125-2:2004 Geographic information -- Simple feature access -- Part 2: SQL option\". International standard.\n",
    "* **[TIM]**: Cox, Simon & Little, Chris (eds.) \"Time Ontology in OWL\". W3C Candidate Recommendation 26 March 2020. https://www.w3.org/TR/owl-time/\n",
    "* **[TMF]**: Car, N.J. 2021. \"RDFlib OWL TIME Functions\". Software library online. https://github.com/rdflib/timefuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0b5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}